{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.getenv(\"ANTHROPIC_API_KEY\")"
      ],
      "metadata": {
        "id": "IYD1-U01rQ-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q youtube-transcript-api langchain-community langchain-openai \\\n",
        "               faiss-cpu tiktoken python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F7iZIBmB8_T",
        "outputId": "e48d7b19-b7bf-48d4-9f04-9c8a4958a460"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/485.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.1/485.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m136.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install libraries"
      ],
      "metadata": {
        "id": "33R54QYjCMAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all LangChain components, model integrations, and utilities\n",
        "!pip install -qU langchain \\\n",
        "                langchain-core \\\n",
        "                langchain-openai \\\n",
        "                langchain-anthropic \\\n",
        "                langchain-community \\\n",
        "                langchain-text-splitters \\\n",
        "                youtube-transcript-api \\\n",
        "                faiss-cpu \\\n",
        "                python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0nhT-odUeEc",
        "outputId": "f0d6d684-e276-4ee4-933e-131ca0ec96a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/388.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "hPcswe0tExci"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1a - Indexing (Document Ingestion)"
      ],
      "metadata": {
        "id": "0ZZrs-ijCTYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "video_id = \"qYNweeDHiyU\"\n",
        "\n",
        "try:\n",
        "    ytb_api = YouTubeTranscriptApi()\n",
        "    transcript_snippets = ytb_api.fetch(video_id)\n",
        "\n",
        "    transcript = \" \".join(snippet.text for snippet in transcript_snippets)\n",
        "\n",
        "    print(\"Success!\")\n",
        "    print(transcript[:300])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p9AXZycFIH6",
        "outputId": "fb07f8a9-0db1-40a4-a4fa-7f1f560b7662"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "everybody's talking about artificial intelligence these days AI machine learning is another Hot Topic are they the same thing or are they different and if so what are those differences and deep learning is another one that comes into play I actually did a video on these three artificial intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "oWSK4-VQH8CG",
        "outputId": "403162a5-3d0f-4261-e3f7-e259d9310409"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"everybody's talking about artificial intelligence these days AI machine learning is another Hot Topic are they the same thing or are they different and if so what are those differences and deep learning is another one that comes into play I actually did a video on these three artificial intelligence machine learning and deep learning and talked about where they fit and there were a lot of comments on that and I read those comments and I'd like to address some of the most frequently asked questions so that we clear up some of the myths and misconceptions around this in addition something else has happened since that video was recorded and that is this the absolute explosion of this area of generative AI things like large language models and chat Bots have seemed to be taking over the world we see them everywhere really interesting technology uh and then also things like deep fakes these are all within the realm of AI but how do they fit within each other how are they related to each other we're going to take a look at that in this video and try to explain how all these Technologies relate and how we can use them first off a little bit of a disclaimer I'm going to have to simplify some of these Concepts in order to not make this video last for a week so those of you that are really deep experts in the field apologies in advance but we're going to try to make this simple and and that will involve some generalizations first of all let's start with AI artificial intelligence is basically trying to simulate with a computer something that would match or exceed human intelligence what is intelligence well it could be a lot of different things but generally we tend to think of it as the ability to learn to infer and to reason things like that so that's what we're trying to do in the broad field of AI of artificial intelligence and if we look at a timeline of AI it really kind of started back around on this time frame and in those days it was very premature most people had not even heard of it uh and uh it basically was a research project but I can tell you uh as an undergrad which for me was back during these times uh we were doing AI work in fact we would use programming languages like lisp uh or prologue uh and these kinds of things uh were kind of the predecessors to what became later expert systems and this was a technology again some of these things existed previous but that's when it really uh hit kind of a critical mass and became more popularized so expert systems of the 1980s maybe in the 90s and and again we use Technologies like this all of this uh was was something that we did before we ever touched in to the next topic I'm going to talk about and that's the area of machine learning machine learning is as its name implies the machine is learning I don't have to program it I give it lots of information and and it observes things so for instance if I start doing this if I give you this and then ask you to predict what's the next thing that's going to be there well you might get it you might not you have very limited training data to base this on but if I gave you one of those and then ask you what to predict would happen next well you're probably going to say this and then you're going to say it's this and then you think you got it all figured out and then you see one of these and then all of a sudden I give you one of those and throw you a curveball so this in fact and then maybe it it goes on like this so a machine learning algorithm is really good at looking at patterns and discovering patterns within data the more training data you can give it the more confident it can be in predicting so predictions are one of the things that machine learning is is particularly good at another thing is spotting outliers like this and saying oh that doesn't belong in it looks different than all the other stuff because the sequence was broken so that's particularly useful in cyber security the area that I work in because we're looking for outliers we're looking for users who are using the system in ways that they shouldn't be or ways that they don't typically do so this technology machine learning is particularly useful for us and machine learning really came along uh and became more popularized uh in this time frame uh in the the 2010s uh and again uh back when I was an undergrad riding my dinosaur to class we were doing this kind of stuff we never once talked about machine learning it might have existed but it really wasn't hadn't hit the popular uh mindset yet uh but this technology has matured greatly over the last few decades and now it becomes the basis of a lot we do going forward the next layer of our Vin diagram involves deep learning well it's deep learning in the sense that with deep learning we use these things called neural networks neural networks are ways that in a computer we simulate and mimic the way the human brain works at least to the extent that we understand how the brain works and it's called Deep because we have multiple layers of those neural networks and the interesting thing about these is they will simulate the way a brain operates but I don't know if you've noticed but human brains can be a little bit unpredictable you put certain things in you don't always get the very same thing out and deep learning is the same way in some cases we're not actually able to fully understand why we get the results we do uh because there are so many layers to the neural network it's a little bit hard to to decompose and figure out exactly what's in there but this has become a very important part and a very important advancement that also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI the most recent advancements in the field of artificial in intelligence all really are in this space the area of generative AI now I'm going to introduce a term that you may not be familiar with it's the idea of foundation models Foundation models is where we get some of these kinds of things for instance an example of a foundation model would be a large language model which is where we take language and we model it and we make predictions in this technology where if I see certain types of of words then I can sort of predict what the next set of words will be I'm going to oversimplify here for the sake of Simplicity but think about this as a little bit like the autoc complete when you start typing something in and then it predicts what your next word will be except in this case with large language models they're not predicting the next word they're predicting the next sentence the next paragraph the next entire document so there's a really an amazing exponential leap in what these things are able to do and we call all of these Technologies generative because they are generating new content um some people have actually made the argument that the generative AI isn't really generative that that these Technologies are really just regurgitating existing information and putting it in different format well let me give you an analogy um if you take music for instance then every note has already been invented so in a sense every song is just a recombination some other permutation of all the notes that already exist already and just putting them in a different order well we don't say new new music doesn't exist people are still composing and creating new songs from the existing information I'm going to say geni is similar it's a it's an analogy so there'll be some imperfections in it but you get the general idea actually new content can be generated out of these and there are a lot of different forms that this can take with other types of models are uh Audio models uh video models and things like that well in fact these we can use to create deep fakes and deep fakes are examples where we're able to take for instance a person's voice and recreate that and then have it seem like the person said things they never said well it's really useful in entertainment situations uh in parities and things like that uh or if someone's losing their voice then you could capture their voice and then they'd be able to type and you'd be able to hear it in their voice but there's also a lot of cases where this stuff could be abused um the chat Bots again come from this space the Deep fakes come from this space but they're all part of generative Ai and all part of these Foundation models and this again is the area that has really caused all of us to really pay attention to AI the possibilities of generating new content or in some cases summarizing existing content and giving us uh something that is bite-size and manageable this is what has gotten all of the attention this is where the chat Bots and all of these things come in in the early days ai's adoption started off pretty slowly most people didn't even know it existed and if they did it was something that always seemed like it was about 5 to 10 years away but then machine learning deep learning and things like that came along and we started seeing some uptake then Foundation models gen Ai and the light came along and this stuff went straight to the Moon these Foundation models are what have changed the adoption curve and now you see AI being adopted everywhere and the thing for us to understand is where this is where it fits in and make sure that we can reap the benefits from all of this technology if you like this video and want to see more like it please like And subscribe if you have any questions or want to share your thoughts about this topic please leave a comment below\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1b - Indexing (Text Splitting)"
      ],
      "metadata": {
        "id": "eKkcYsaOCrRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.create_documents([transcript])"
      ],
      "metadata": {
        "id": "24i-ZSVXFbnC"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dm9sfpQFnF1",
        "outputId": "fc6a5e06-b078-4e2d-a3fe-b13c0ba7f89f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYlrcBrkFO-N",
        "outputId": "3d5d3707-02a6-4104-a9eb-3bc32b4b9b54"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content=\"we're looking for users who are using the system in ways that they shouldn't be or ways that they don't typically do so this technology machine learning is particularly useful for us and machine learning really came along uh and became more popularized uh in this time frame uh in the the 2010s uh and again uh back when I was an undergrad riding my dinosaur to class we were doing this kind of stuff we never once talked about machine learning it might have existed but it really wasn't hadn't hit the popular uh mindset yet uh but this technology has matured greatly over the last few decades and now it becomes the basis of a lot we do going forward the next layer of our Vin diagram involves deep learning well it's deep learning in the sense that with deep learning we use these things called neural networks neural networks are ways that in a computer we simulate and mimic the way the human brain works at least to the extent that we understand how the brain works and it's called Deep\")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)"
      ],
      "metadata": {
        "id": "8xYFK7WXC2Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U voyageai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbNkprIbcnaW",
        "outputId": "69b94d07-7b9f-4f3d-c135-8039ede2c90e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: voyageai in /usr/local/lib/python3.12/dist-packages (0.3.7)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from voyageai) (3.13.2)\n",
            "Requirement already satisfied: aiolimiter in /usr/local/lib/python3.12/dist-packages (from voyageai) (1.2.1)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.12/dist-packages (from voyageai) (0.2.0)\n",
            "Requirement already satisfied: langchain-text-splitters>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from voyageai) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from voyageai) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from voyageai) (11.3.0)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from voyageai) (2.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from voyageai) (2.32.5)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.12/dist-packages (from voyageai) (9.1.2)\n",
            "Requirement already satisfied: tokenizers>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from voyageai) (0.22.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters>=0.3.8->voyageai) (1.2.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.8->voyageai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.8->voyageai) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.8->voyageai) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.8->voyageai) (0.4.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.14.0->voyageai) (0.36.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai) (1.22.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python->voyageai) (1.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->voyageai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->voyageai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->voyageai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->voyageai) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (6.0.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (0.4.59)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (0.12.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters>=0.3.8->voyageai) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-voyageai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXiIuoX2euKR",
        "outputId": "f84031e4-9ca9-4868-e57c-231d0222b395"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-voyageai\n",
            "  Downloading langchain_voyageai-0.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: langchain-core>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-voyageai) (1.2.5)\n",
            "Requirement already satisfied: voyageai<1,>=0.3.4 in /usr/local/lib/python3.12/dist-packages (from langchain-voyageai) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-voyageai) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0->langchain-voyageai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0->langchain-voyageai) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0->langchain-voyageai) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0->langchain-voyageai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0->langchain-voyageai) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0->langchain-voyageai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=1.0.0->langchain-voyageai) (0.12.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-voyageai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-voyageai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-voyageai) (0.4.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from voyageai<1,>=0.3.4->langchain-voyageai) (3.13.2)\n",
            "Requirement already satisfied: aiolimiter in /usr/local/lib/python3.12/dist-packages (from voyageai<1,>=0.3.4->langchain-voyageai) (1.2.1)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.12/dist-packages (from voyageai<1,>=0.3.4->langchain-voyageai) (0.2.0)\n",
            "Requirement already satisfied: langchain-text-splitters>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from voyageai<1,>=0.3.4->langchain-voyageai) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from voyageai<1,>=0.3.4->langchain-voyageai) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from voyageai<1,>=0.3.4->langchain-voyageai) (11.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from voyageai<1,>=0.3.4->langchain-voyageai) (2.32.5)\n",
            "Requirement already satisfied: tokenizers>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from voyageai<1,>=0.3.4->langchain-voyageai) (0.22.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=1.0.0->langchain-voyageai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0->langchain-voyageai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0->langchain-voyageai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0->langchain-voyageai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0->langchain-voyageai) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->voyageai<1,>=0.3.4->langchain-voyageai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->voyageai<1,>=0.3.4->langchain-voyageai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->voyageai<1,>=0.3.4->langchain-voyageai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->voyageai<1,>=0.3.4->langchain-voyageai) (2025.11.12)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.14.0->voyageai<1,>=0.3.4->langchain-voyageai) (0.36.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai<1,>=0.3.4->langchain-voyageai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai<1,>=0.3.4->langchain-voyageai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai<1,>=0.3.4->langchain-voyageai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai<1,>=0.3.4->langchain-voyageai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai<1,>=0.3.4->langchain-voyageai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai<1,>=0.3.4->langchain-voyageai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->voyageai<1,>=0.3.4->langchain-voyageai) (1.22.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python->voyageai<1,>=0.3.4->langchain-voyageai) (1.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0->langchain-voyageai) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0->langchain-voyageai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=1.0.0->langchain-voyageai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.14.0->voyageai<1,>=0.3.4->langchain-voyageai) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.14.0->voyageai<1,>=0.3.4->langchain-voyageai) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.14.0->voyageai<1,>=0.3.4->langchain-voyageai) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.14.0->voyageai<1,>=0.3.4->langchain-voyageai) (1.2.0)\n",
            "Downloading langchain_voyageai-0.3.0-py3-none-any.whl (6.6 kB)\n",
            "Installing collected packages: langchain-voyageai\n",
            "Successfully installed langchain-voyageai-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_voyageai import VoyageAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = VoyageAIEmbeddings(\n",
        "    model=\"voyage-3.5-lite\",\n",
        ")\n",
        "\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)\n"
      ],
      "metadata": {
        "id": "jYXeS5T7FrC4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.index_to_docstore_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWYkp-NmFSVF",
        "outputId": "dc274525-9b98-4018-89e2-be579c2a6ba4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'd70ec85d-6294-4036-951d-6a32014cf74c',\n",
              " 1: '59c661a2-85e5-4145-88be-24dacd7466ba',\n",
              " 2: '7f332610-5eef-4005-951d-c5a4d8c5fe27',\n",
              " 3: '6fc50312-bbc4-4e07-990e-1e00bd3d849a',\n",
              " 4: 'a5126711-efc6-4cae-b27b-bd29544dda1c',\n",
              " 5: 'e4d78935-7a52-4d11-8474-8ae794be07b9',\n",
              " 6: '1c041432-199a-4e58-9513-5dcf19a8b739',\n",
              " 7: '5fd73107-2b3a-4a2c-a917-4e32228d982a',\n",
              " 8: '6b16532b-0461-4ace-a3f3-ba9d50b90f59',\n",
              " 9: '77bdfb14-d3f2-4ad8-8e76-07b3edc25313',\n",
              " 10: '293b42d7-aa26-47b9-b0db-ecee228b854c',\n",
              " 11: '615ef3b3-45a6-4f7b-bf08-4a76fe77d840'}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.get_by_ids(['5fd73107-2b3a-4a2c-a917-4e32228d982a'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxokTcWEGGAo",
        "outputId": "67eb4cee-094c-47df-8045-d43a515605c2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='5fd73107-2b3a-4a2c-a917-4e32228d982a', metadata={}, page_content=\"important advancement that also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI the most recent advancements in the field of artificial in intelligence all really are in this space the area of generative AI now I'm going to introduce a term that you may not be familiar with it's the idea of foundation models Foundation models is where we get some of these kinds of things for instance an example of a foundation model would be a large language model which is where we take language and we model it and we make predictions in this technology where if I see certain types of of words then I can sort of predict what the next set of words will be I'm going to oversimplify here for the sake of Simplicity but think about this as a little bit like the autoc complete when you start typing something in and then it predicts what your next word will be except in this case with large language models they're not predicting the next\")]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Retrieval"
      ],
      "metadata": {
        "id": "Zez1650EDN9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "KEuoGUYOF3oG"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcose8VuGFAv",
        "outputId": "32f5e759-ddc0-4cec-e28e-d97f475568e8"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'VoyageAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7e671dfffe00>, search_kwargs={'k': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke('What is generative ai')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvrsq08TGGNk",
        "outputId": "f4d6ed6b-1a32-4ee0-9874-963e05d55648"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='6b16532b-0461-4ace-a3f3-ba9d50b90f59', metadata={}, page_content=\"a little bit like the autoc complete when you start typing something in and then it predicts what your next word will be except in this case with large language models they're not predicting the next word they're predicting the next sentence the next paragraph the next entire document so there's a really an amazing exponential leap in what these things are able to do and we call all of these Technologies generative because they are generating new content um some people have actually made the argument that the generative AI isn't really generative that that these Technologies are really just regurgitating existing information and putting it in different format well let me give you an analogy um if you take music for instance then every note has already been invented so in a sense every song is just a recombination some other permutation of all the notes that already exist already and just putting them in a different order well we don't say new new music doesn't exist people are still\"),\n",
              " Document(id='5fd73107-2b3a-4a2c-a917-4e32228d982a', metadata={}, page_content=\"important advancement that also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI the most recent advancements in the field of artificial in intelligence all really are in this space the area of generative AI now I'm going to introduce a term that you may not be familiar with it's the idea of foundation models Foundation models is where we get some of these kinds of things for instance an example of a foundation model would be a large language model which is where we take language and we model it and we make predictions in this technology where if I see certain types of of words then I can sort of predict what the next set of words will be I'm going to oversimplify here for the sake of Simplicity but think about this as a little bit like the autoc complete when you start typing something in and then it predicts what your next word will be except in this case with large language models they're not predicting the next\")]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 - Augmentation"
      ],
      "metadata": {
        "id": "F8y0wRmoDSVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\",temperature=0.2,max_tokens=100)"
      ],
      "metadata": {
        "id": "x2P2AlJ0GN5L"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "      You are a helpful assistant.\n",
        "      Answer ONLY from the provided transcript context.\n",
        "      If the context is insufficient, just say you don't know.\n",
        "\n",
        "      {context}\n",
        "      Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables = ['context', 'question']\n",
        ")"
      ],
      "metadata": {
        "id": "2-NeLx9wFHzw"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question          = \"What is generative Ai?\"\n",
        "retrieved_docs    = retriever.invoke(question)"
      ],
      "metadata": {
        "id": "WI9BOZQwGizf"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfv8yNFsK_GN",
        "outputId": "e9d332ee-e65d-4599-97ea-d28a04f832e4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='6b16532b-0461-4ace-a3f3-ba9d50b90f59', metadata={}, page_content=\"a little bit like the autoc complete when you start typing something in and then it predicts what your next word will be except in this case with large language models they're not predicting the next word they're predicting the next sentence the next paragraph the next entire document so there's a really an amazing exponential leap in what these things are able to do and we call all of these Technologies generative because they are generating new content um some people have actually made the argument that the generative AI isn't really generative that that these Technologies are really just regurgitating existing information and putting it in different format well let me give you an analogy um if you take music for instance then every note has already been invented so in a sense every song is just a recombination some other permutation of all the notes that already exist already and just putting them in a different order well we don't say new new music doesn't exist people are still\"),\n",
              " Document(id='5fd73107-2b3a-4a2c-a917-4e32228d982a', metadata={}, page_content=\"important advancement that also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI the most recent advancements in the field of artificial in intelligence all really are in this space the area of generative AI now I'm going to introduce a term that you may not be familiar with it's the idea of foundation models Foundation models is where we get some of these kinds of things for instance an example of a foundation model would be a large language model which is where we take language and we model it and we make predictions in this technology where if I see certain types of of words then I can sort of predict what the next set of words will be I'm going to oversimplify here for the sake of Simplicity but think about this as a little bit like the autoc complete when you start typing something in and then it predicts what your next word will be except in this case with large language models they're not predicting the next\")]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "context_text"
      ],
      "metadata": {
        "id": "bKwpvAo5G_Pk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "832ba275-56b7-4b1e-9406-bb91803ab0ee"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"a little bit like the autoc complete when you start typing something in and then it predicts what your next word will be except in this case with large language models they're not predicting the next word they're predicting the next sentence the next paragraph the next entire document so there's a really an amazing exponential leap in what these things are able to do and we call all of these Technologies generative because they are generating new content um some people have actually made the argument that the generative AI isn't really generative that that these Technologies are really just regurgitating existing information and putting it in different format well let me give you an analogy um if you take music for instance then every note has already been invented so in a sense every song is just a recombination some other permutation of all the notes that already exist already and just putting them in a different order well we don't say new new music doesn't exist people are still\\n\\nimportant advancement that also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI the most recent advancements in the field of artificial in intelligence all really are in this space the area of generative AI now I'm going to introduce a term that you may not be familiar with it's the idea of foundation models Foundation models is where we get some of these kinds of things for instance an example of a foundation model would be a large language model which is where we take language and we model it and we make predictions in this technology where if I see certain types of of words then I can sort of predict what the next set of words will be I'm going to oversimplify here for the sake of Simplicity but think about this as a little bit like the autoc complete when you start typing something in and then it predicts what your next word will be except in this case with large language models they're not predicting the next\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})"
      ],
      "metadata": {
        "id": "_bikWKZWDiqB"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LOFVVAbLYvU",
        "outputId": "4fdb184a-cacd-47e6-8e46-047a9a096012"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text=\"\\n      You are a helpful assistant.\\n      Answer ONLY from the provided transcript context.\\n      If the context is insufficient, just say you don't know.\\n\\n      a little bit like the autoc complete when you start typing something in and then it predicts what your next word will be except in this case with large language models they're not predicting the next word they're predicting the next sentence the next paragraph the next entire document so there's a really an amazing exponential leap in what these things are able to do and we call all of these Technologies generative because they are generating new content um some people have actually made the argument that the generative AI isn't really generative that that these Technologies are really just regurgitating existing information and putting it in different format well let me give you an analogy um if you take music for instance then every note has already been invented so in a sense every song is just a recombination some other permutation of all the notes that already exist already and just putting them in a different order well we don't say new new music doesn't exist people are still\\n\\nimportant advancement that also reached some popularity during the 2010s and as something that we use still today as the basis for our next area of AI the most recent advancements in the field of artificial in intelligence all really are in this space the area of generative AI now I'm going to introduce a term that you may not be familiar with it's the idea of foundation models Foundation models is where we get some of these kinds of things for instance an example of a foundation model would be a large language model which is where we take language and we model it and we make predictions in this technology where if I see certain types of of words then I can sort of predict what the next set of words will be I'm going to oversimplify here for the sake of Simplicity but think about this as a little bit like the autoc complete when you start typing something in and then it predicts what your next word will be except in this case with large language models they're not predicting the next\\n      Question: What is generative Ai?\\n    \")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 - Generation"
      ],
      "metadata": {
        "id": "MxxcV2C_DXqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = llm.invoke(final_prompt)\n",
        "print(answer.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX6vxSoUHBok",
        "outputId": "a3e287c6-2624-4b1b-89d9-67fbfe708c60"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, generative AI refers to large language models that can generate new content, such as predicting the next sentence, paragraph, or entire document, rather than just predicting the next word like autocomplete. The context suggests that generative AI is seen as a significant advancement in AI capabilities, going beyond simply recombining existing information into new formats. However, the context does not provide a detailed definition or explanation of generative AI, so I cannot give a more comprehensive answer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Chain"
      ],
      "metadata": {
        "id": "wH2Ph0NcDlo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "RdTwSS3nHKRz"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ],
      "metadata": {
        "id": "VGezE1qYQJ76"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain = RunnableParallel({\n",
        "    'context': retriever | RunnableLambda(format_docs),\n",
        "    'question': RunnablePassthrough()\n",
        "})"
      ],
      "metadata": {
        "id": "fmYnYqbWQWLi"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain.invoke(\"What is AI?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGI1hEvfQvLb",
        "outputId": "e41970e1-1060-4eeb-d5ae-4394db901d77"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': \"we see them everywhere really interesting technology uh and then also things like deep fakes these are all within the realm of AI but how do they fit within each other how are they related to each other we're going to take a look at that in this video and try to explain how all these Technologies relate and how we can use them first off a little bit of a disclaimer I'm going to have to simplify some of these Concepts in order to not make this video last for a week so those of you that are really deep experts in the field apologies in advance but we're going to try to make this simple and and that will involve some generalizations first of all let's start with AI artificial intelligence is basically trying to simulate with a computer something that would match or exceed human intelligence what is intelligence well it could be a lot of different things but generally we tend to think of it as the ability to learn to infer and to reason things like that so that's what we're trying to do\\n\\nwhat is intelligence well it could be a lot of different things but generally we tend to think of it as the ability to learn to infer and to reason things like that so that's what we're trying to do in the broad field of AI of artificial intelligence and if we look at a timeline of AI it really kind of started back around on this time frame and in those days it was very premature most people had not even heard of it uh and uh it basically was a research project but I can tell you uh as an undergrad which for me was back during these times uh we were doing AI work in fact we would use programming languages like lisp uh or prologue uh and these kinds of things uh were kind of the predecessors to what became later expert systems and this was a technology again some of these things existed previous but that's when it really uh hit kind of a critical mass and became more popularized so expert systems of the 1980s maybe in the 90s and and again we use Technologies like this all of this uh\",\n",
              " 'question': 'What is AI?'}"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "e6osgdBfRCPN"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain = parallel_chain | prompt | llm | parser"
      ],
      "metadata": {
        "id": "Y3e2en89QyOC"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain.invoke('Can you summarize the video')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Ur7Ph_xlRE-7",
        "outputId": "7ead8406-9c0f-4d7b-bd57-f032da457d8e"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided transcript, here is a summary:\\n\\nThe video discusses how various AI technologies like chatbots, deep fakes, and generative AI are related to each other and to the broader field of artificial intelligence (AI). \\n\\nThe key points are:\\n\\n- AI is the attempt to simulate human-level intelligence in computers, including the ability to learn, infer, and reason.\\n\\n- Generative AI models, which can create new content, have been a major'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZyERl2UwRKn6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}